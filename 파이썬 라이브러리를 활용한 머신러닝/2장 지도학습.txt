지도학습에는 회귀와 분류가 있다.

분류는 미리 정의된, 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것입니다.

분류는 이진분류와 다중 분류로 나뉩니다. 이진 분류는 양성분류와 음성 분류로 나뉨

회귀는 연속적인 숫자 또는 프로그래밍 용어로 말하면 부동소수점수(수학 용어로는 실수)를 예측 하는 것 범위 예측도 포함 연속성으로 구분하면 됨 

모델이 처음보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 데스트 세트로 일반화(generalization) 되었다고 한다.

가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것을 과대적합이라고 한다.

너무 간단한 모델일 선택되는 것을 과소적합이라고 한다.

scikit-learn에 포함된 데이터셋은 Bunch 객체에 저장되어있음

Bunch 객체는 dictionary와 비슷하지만 점 표기법을 사용할 수있다.

결정 계수는 R제곱 값이라고도 하는데 회귀 일때 결졍계수를 반환한다.

결정계수는 회귀 모델에서 예측의 적합도를 측정한 것으로 0에서 1사이 값을 가짐

k-NN 의 장점은 쉬운 모델 단점은 데이터셋이 크면 시간이 오래 걸림 



머신러닝에서 알고리즘이 주어진 데이터로부터 학습하는 파라미터를 흔히 모델 파라미더라고 부름

모델이 학습할 수 없어서 사람이 직접 설정해 주어야 하는 파라미터를 하이퍼파라미터라고 부름

하이퍼파라미터는 파이썬 클래스와 함수에 넘겨주는 인수에 포함되므로 통칭하여 매개변수로 부름

선형회귀 도는 최소제곱법이라고 불리는 것은 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차를 최소화하는 파라미터 w와b를 찾는 것이다.

평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.

선형 회귀는 매개변수가 없는 것이 장점이지만 그래서 복잡도를 제어하지 못한다

기울기 파리미터(w)는 가중치(weight) 또는 계수(coefficient)라고 하며 lr 객체의 coef_ 속성에 저장되어 있고 편향(offset) 또는 절편(intercept)파리미터(b)는 intercept_ 속성에 저장되어있다.

intercept_ 속성은 항상 실수(float)값 하나지만, coef_속성은 각 입력 특성에 하나씩 대응되는 Numpy 배열이다.

기본 선형 회귀 식은 복잡도를 제어 할 수 없다

리지(Ridge)도 회귀를 위한 선형 모델이므로 최소적합법에서 사용한 것과 같은 예측 함수를 사용한다.

리지 회귀에서의 가중치(w) 선택은 훈련 데이터를 잘 예측하기 위해서 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적도 있다.

가중치의 절댓값을 가능한 한 작게 만드는 것이 목표이다.

w의 모든 원소가 0에 가깝게 되는 것이 목표, 이는 모든 특성이 출력에 주는 영향을 최소한으로 만드는 것이다. 이런 제약을 규제(regularization)이라고 한다.

규제란 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미이다.

리지 회귀에서 사용하는 규제 방식을 L2규제라고 한다.

수학적으로 리지는 계수의 L2노름(norm)의 제곱을 패널티로 적용한다.

Ridge 모델에서 alpha 매개변수로 훈련세트의 성능 대비 모델을 얼마나 단순화할지 지정 할 수 있다. alpha값을 높이면 계수를 0에 더 가깝게 만들어서 훈련세트의 성능은 나빠지지만 일반화에 도움을 줄 수 있다.

라소(lasso)도 리지 회귀와 같이 계수를 0에 가깝게 만들려고 한다. 이를 L1규제라고 한다. 라소는 어떤 계수를 정말 0으로 만들어서 모델에서 완전히 제거 시킨다.

라소와 리지 중 지리를 더 선호 하지만 특성이 많고 그중 일부분만 중요하다면 라소가 더 좋다. 라소와 리지를 결합한 ElasticNet도 있다.ElasticNet은 매개변수 두개를 조정해야한다.

회귀용 선형 모델에서는 출력이 특성의 선형 함수였다. 분류용 선형 모델에서는 결정 경계까 입력의 선형 함수이다.

손실함수(loss fuction)에 대한 차이는 중요하지 않다.

많이 알려진 두개의 선형 분류 알고리즘은 로지스틱 회귀(Logistic regression)과 서포트 벡터 머신(supoort vector machine)이다. 

Ridge와 마찬가지로 로지스틱 회귀와 서포트 벡터 머신은 L2 규제를 사용한다.

규제의 강도를 결정하는 매개 변수는 C이다. C값이 높아지면 규제가 감소한다. 즉, 매개변수로 높은 C값을 지정하면 훈련세트에 가능한 최대로 맞추려 노력하고, 반면에 C값을 낮추면 모델은 계수 벡터(w)가 0에 가까워지도록 만든다.

C의 값이 낮아지면 데이터 포인트 중 다수에 맞추려고 하는 반면, C의 값을 높이면 개개의 데이터 포인트를 정확히분류하려고 노력한다. 

선형 모델의 주요 매개 변수는 회귀 모델에서는 alpha 였고 LinearSVC와 LogisticRegression에서는 C이다. alpha 값이 클수록, C값이 작을수록 모델이 단순해 진다. 회귀 모델에서 이 매개 변수를 조정하는 일이 매주 종요하다. 중요한 특성이 많지 않다고 생각하면 L1구제를 사용한다. 그러지않으면 기본적으로 L2 규제를 사용한다. 

나이브 베이즈(naive bayes) 분류기는 로지스틱 리그레션이나 서포터 벡터 머신 같은 선형 분류기보다 훈련속도가 빠른 편이지만 그대신 일반화 성능이 조금 뒤쳐진다.

나이브 베이즈 분류기가 효과적인 이유는 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합하기 때문이다.

나이브 베이즈 분류기는 GaussianNB, BernoulliNB, MultinomialNB 세가지다.

가우시안은 연속적인 어떤데이터에도, 베노울리는 이진 데이터, 멀티노미얼은 카운트 데이터에 적용된다.

decision tree 는 스무고개 형식이다. 땅따먹기 형식으로 루트노드에서 시작하여 분할된 영역이 한개의 타깃값만 가질때까지 계속 나눈다. 하나의 타깃으로만 이루어진 노드를 순수 노드라고한다. 

결정트리에서 과대적합을 막는 방법은 사전가지치기와 사후가지치기 또는 가지치기

결정 트리는 만들어진 모델을 쉽게 시각화 할 수 있어서 비전문가도 이해하기 쉽다. 데이터의 스케일에 구애받지 않는다. 단점은 과대적합되는 경향이 있어 일반화 성능이 좋지 않다. 그래서 양상불 방법을 단일결정트리의 대안으로 사용한다.

앙상블ㅇ느 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법이다.

랜덤포레스트와 그래디언트 부스팅 결정트리는 둘 다 모델을 구성하는 기본요소로 결정트리를 사용한다.

랜덤 포레스트는 기본적으로 조금씩 다른 여러 결정 트리의 묶음이다. 각 트리는 비교적 예측을 잘 할 수 있지만 데이터의 일부에 과대적합하는 경향을 가지기 때문에 잘 작동하되 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 과대적합된 양을 줄일 수 있다.

부트스트랩 샘플은 랜덤포레스트를 만들 때 데이터 포인트 중에서 무작위로 데이터를 추출하는 것이다.

회귀와 분류에 있어서 랜덤 포레스트는 현재 가장 널리 사용되는 머신러닝 알고리즘이다.

그래디언트부스팅트리는 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만든다. 기본적으로 무작위성이 없다. 대신 강력한 사전 가지치기를 사용한다. 근본 아이디어는 약한 학습기를 많이 연결하는 것이다. 

그래디언트 부스팅 결정 트리는 지도 학습에서 가장 강력하고 널리 사용하는 모델 중 하나다. 단점은 매개변수를 잘 조정해야 한다는 것과 훈련 시간이 길다는 것이다. 매개변수는 트리의 개수를 지정하는 n_estimators와 이전 트리의 오차를 보정하는 정도를 조절하는 learning_rate이다. 두 매개변수는 깊게 연관되며 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해서 더 많은 트리를 추가 해야 한다. n_estimators가 클수록 좋은 랜덤 포레스트와 달리 그래디언트 부스팅에서 n_estimators를 크게 하면 모델이 복잡해지고 과대적합될 가능성이 높아진다. 일반적인 관례는 가용한 시간과 메모리 한도 내에서 트리의 개수를 맞추고 학습률을 조정한다. 

배깅(Bagging)은 Bootstrap aggregating의 줄임말이다. 배깅은 중복을 허용한 랜덤 샘플링으로 만든 훈련 세트를 사용하여 분류기를 각각 다르게 학습시킵니다. predict_proba() 매서드를 지원하는 경우 확률값을 평균하여 예측을 수행한다. 그렇지 않은 분류기를 사용할 때는 가장 빈도가 높은 클래스 레이블이 예측 결과가 된다.

엑스트라 트리(Extra_Trees)는 랜텀 포레스트와 비슷하지만 후보 특성을 무작위로 분할한 다음 최적의 분할을 찾는다. 랜덤 포레스트와 달리 DecisionTreeClassifier(spliter='random')을 사용하고 부트스트랩 샘플링을 적용하지 않는다. 무작위성을 증가시키면 일반적으로 모델의 편향이 늘어나지만 분산이 감소한다. 엑스트라 트리와 랜덤 포레스트는 다른 방식으로 모델에 무작위성을 주입한다고 볼 수 있다. 예측 방식은 랜덤 포레스트와 동일하게 각 트리가 만든 확률값을 평균한다.

에이다 부스트(AdaBoost)는 Adaptive Boosting 의 줄임말입니다. 에이다 부스트는 그래디언트 부스팅 처럼 약한 학습기를 사용합니다. 그래디언트 부스팅과는 달리 이전의 모델이 잘 못 분류한 샘플에 가중치를 높여서 다음 모델에 훈련시킵니다. 훈련된 각 모델은 성능에 따라 가중치가 부여됩니다. 예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.

커널 서포트 벡터 머신은 SVM으로 불린다. 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한것이다. 분류와 회귀에 적용 가능하다. 

커널 기법은 어떤 특성을 추가해야 할지 모르고 특성을 많이 추가하면 연산비용이 커진다. 이럴때, 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리(더 정확히는 스칼라 곱)를 계산한다. 서포트 벡터 머신에서 데이터를 고차원 공간으로 매핑하는 데 많이 사용하는 방법은 두가지 이다. 원래 특성의 가능한 조합을 지정된 차수까지 모두 계산(예를 들어 특성 1제곱 곱하기 특성 2오제곱)하는 다항식 커널이 있고 가우시안(gaussian)커널로도 불리는 RBF(radial basis function)커널이 있다. 

가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다. 가우시안 커널은 모든 차수의 모든 다항식을 고려한다.  하지만 특성의 중요도는 고차항이 될 수록 줄어든다.

학습이 진행되는 동안 SVM은 각 훈련 데이터 포인트가 두 클래스 사이의 결정 경계를 구분하는 데 얼마나 중요한지를 배우게 된다. 두 클래스 사이의 경계에 위치한 데이터 포인트만 결정경계를 만드는데 영향을 준다. 이 데이터 포인트를 서포트 벡터(support vector)라고 부른다. SVM 모델은 데이터의 자릿수에 크게 영향을 받는다.

gamma 매개변수는 r로 가우시안 커널 폭의 역수에 해당된다. gamma 매개변수가 하나의 훈련 샘플이 미치는 영향을 결정한다. 작은 값은 넓은 영역을 뜻하며 큰값이라면 영향이 미치는 범위가 제한된다. 가우시안 커널의 반경이 클수록 훈련 샘플의 영향범위도 커진다. C 매개 변수는 선형 모델에서 사용한 것과 비슷한 규제 매개변수이다. 이 매개변수는 각 포인트의 중요도(정확히는 dual_coef_값)을 제한한다. 

커널 서포트 벡터 머신은 강력한 모델이며 다양한 데이터셋에서 잘 작동한다. SVM은 데이터의 특성이 몇 개 안되더라도 복잡한 결정경계를 만들 수 있다. 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많을 때는 잘 맞지 않는다. 만개의 샘플 정도에서 잘작동하지만 십만개 이상에서는 별로다.

SVM의 또하나의 단점은 데이터 전처리와 매개변수 설정에 신경을 많이 써야 한다는 점이다. 그래서 랜덤포레스트나 그래디언트 부스팅 같은 (전처리가 거의 필요없는) 트리 기반 모델을 애플리케이션에서 많이 사용한다. 분석하기도 어렵고 예측이 어떻게 결정되었는지 이해하기도 어렵다.

모든 특성이 비슷한 단위이고 스케일이 비슷하면 시도 해볼만하다.

커널 SVM에서 중요한 매개변수는 규제 매개 변수 C이고 어떤 커널을 사용할 지와 각 커널에 따른 매개변수이다. RBF 커널은 가우시안 커널 폭의 역수인 gamma 매개 변수 하나를 가집니다. gamma와 C 모두 모델의 복잡도를 조정하며 둘 다 큰값이 더 복잡한 모델을 만듭니다. 

다층 퍼셉트론(multilayer perceptrons, MLP)는 기본 피드포워드(feed-forward)신경망, 또는 그냥 신경망이라고도 한다. 입력, 출력, 은닉층의 유닛들이 모두 연결되어 있다고 하여 완전 연결 신경망이라고도 한다.

MLP는 여러 단계를 거쳐 결정을 만들어내는 선형 모델의 일반화된 모습이다.

MLP에서는 가중치 합을 만드는 과정이 여러번 반복되며, 먼저 중간 단계를 구성하는 은익 유닛(hidden unit)을 계산하고 이를 이용하여 최종결과를 산출하기 위해 다시 가중치 합을 계산한다.

MLP은 많은 계수(또는 가중치)를 학습해야 한다. 이 계수는 각 입력과 은닉층(hidden layer)의 은닉 유닛 사이, 그리고 각 유닛과 출력 사이마다 있다. 

여러개의 가중치 합을 계산하는 것은 수학적으로 보면 하나의 가중치 합을 계산하는 것과 같다.MLP을 선형 모델보다 강력하게 만들려면 각 은닉 유닛의 가중치 합을 계산한 후 그 결과에 비선형 함수인 렐루(rectified linear unit. ReLU)나 하이퍼볼릭 탄젠트(hyperbolic tangent, tanh)를 적용한다. 이런 함수들을 활성화 함수(activation function)이라고 하며 이외에도 시그모이드(sigmoid)함수가 있다.

시그모이드 함수는 로지스틱 함수로도 불린다.

렐루 함수는 0 이하를 잘라버리고, tanh 함수는 낮은 입력값에대해서 -1로 수렵하고 큰 입력값에 대해서는 +1로 수렴한다. 

많은 은닉층으로 구성된 대규모의 신경망이 생기면서 이를 딥러닝이라고 부르게 되었다.

MLP는 SVC 처럼 데이터의 스케일이 영향을 미친다. 신경망도 모든 입력 특성을 평균은 0, 분산은 1이 되도록 변형하는 것이 좋다. 데이터에서 평균을 빼고 표준편차로 나눈 값을 z-점수(z-score)또는 표준 점수(standard score)라고 합니다. z-점수는 평균이 0 분산이 1인 표준정규분포이다.

신경망은 머신러닝 분야의 많은 애플리케이션에서 최고의 모델로 다시 떠오르고 있다. 장점은 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 만들 수 있다는 점이다. 충분한 연산 시간과 데이터를 주고 매개변수를 세심하게 조정하면 신경망은 (분류와 회귀 문제에 모두) 종종 다른 머신러닝 알고리즘을 뛰어넘는 성능을 낸다. 

신경망의 단점은 학습이 오래걸리고 데이터 전처리에 주의 해야 한다. 신경망의 매개변수를 조정하는 일반적인 방법은 먼저 충분히 과대적합되어서 문제를 해결할만한 큰 모델을 만든다. 그런 다음 훈련 데이터가 충분히 학습될 수 있다고 생각 될때 신경망 구조를 줄이거나 규제 강화를 위해 alpha값을 증가시켜 일반화 성능을 향상 시킨다.

adam은 대부분에 데이터에서 잘 작동하지만 데이터의 스케일에 민감하다.

lbgfs는 안정적이지만 규모가 큰 모델이나 대량의 데이터셋에서는 시간이 오래 걸린다. 

solver 매개변수를 'adam' 또는 'sgd'로 두고 전체 데이터를 일전크기로 나눈 미니 배치(mini-batch)를 사용하여 모델을 점진적으로 학습시킬 경우가 있습니다. 전체 데이터를 메모리에 모두 적재할 수 없을 때는 fit 메서드 대신에 학습된것을 유지하면서 반복 하여 학습할 수 있는 partial_fit 메서드를 사용한다.

decision_function(결정 함수) 과 predict_proba(예측 확률)는 scikit_learn 에 있는 분류기에서 불화실성을 추정할 수가 있는 함수다.

이진 분류에서 decision_function 반환값의 크기는 (n_samples)이며 각 샘플이 하나의 실수 값을 반환한다.

predict_proba 의 출력은 각 클래스에 대한 확률이고, decision_function의 출력보다 이해하기 더 쉽다. 이 값의 크기는 이진 분류에서는 항상 (n_samples, 2)이다.

과대적합된 모델은 혹 잘못된 예측이더라도 예측의 확신이 강한 편이다. 일반적으로 복잡도가 낮은 모델은 예측에 불확시성이 더 많다. 이런 불확실성과 모델의 정확도가 동등하면 이 모델이 보정(calibration)되었다고 한다. 즉 보정된 모델에서 70% 확신을 가진 예측은 70%의 정확도를 낼 것이다.

다중 분류에서 decision_function의 결괏값의 크기는 (n_samples, n_class)이다. 각열은 각 클래스에 대한 확신 점수를 담고 있다. 데이터 포인트마다 점수들에서 가증 큰 값을 찾아 예측 결과를 재현할 수 있다.

최근접 이웃 - 작은 데이터 셋일 경우, 기본 모델로서 좋고 설명하기 쉬움

선형모델 - 첫 번째로 시도할 알고리즘, 대용량 데이터셋 가능, 고차원 데이터에 가능

나이브 베이즈 - 분류만 가능, 선형모델보다 훨씬 빠름, 대용량 데이터셋과 고차원 데이터에 가능, 선형 모델보다 덜 정확함

결정 트리 - 매우 빠름, 데이터 스케일 조정이 필요 없음, 시각화하기 좋고 설명하기 좋음

랜덤 포레스트 - 결정트리 하나보다 거의 항상 좋은 성능을 냄. 매우 안정적이고 강력함. 데이터 스케일 조정필요 없음. 고차원 회소 데이터에는 잘안 맞음

그래디언트 부스팅 결정 트리 - 랜덤 포레스트보다 조금 더 성능이 좋음. 랜덤 포레스트보다 학습은 느리나 예측은 빠르고 메모리를 조금 사용. 랜덤 포레스트보다 매개변수 튜닝이 많이 필요함

서포트 벡터 머신 - 비슷한 의미의 특성으로 이뤄진 중간 규모 데이터셋에 잘 맞음. 데이터 스케일 조정 필요, 매개 변수에 민감

신경망 - 특별히 대용량 데이터셋에서 매우 복잡한 모델을 만들 수 있음 매개변수 선택과 데이터 스케일에 민감 큰모델은 학습이 오래 걸림

새로운 데이터셋으로 작업할 대는 선형 모델이나 나이브 베이즈 또는 최근접 이웃 분류기 같은 간단한 모델로 시작해서 성능이 얼마나 나오는지 가늠해보는 것이 좋다. 데이터를 충분히 이해한 뒤에 랜덤 포레스트나 그래디언트 부스팅 결정 트리, SVM, 신경망 같은 복잡한 모델을 만들 수 있는 알고리즘을 고려해 볼 수 있다.

