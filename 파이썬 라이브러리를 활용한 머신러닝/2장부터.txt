from IPython.display import display

import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import pandas as pd

import mglearn

------------------------------------------------------------------------------------------------------

import platform

from matplotlib import font_manager, rc
plt.rcParams['axes.unicode_minus']=False

if platform.system()=='Darwin':
    rc('font', family='AppleGothic')
elif platform.system() =='Windows':
    path = 'c:/Windows/Fonts/malgun.ttf'
    font_name = font_manager.FontProperties(fname=path).get_name()
    rc('font', family=font_name)
else:
    print('Unknown system... sorry~~~~')

----------------------------------------------------------------------------------------------------------------

지도학습에는 회귀와 분류가 있다.

분류는 미리 정의된, 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것입니다.

분류는 이진분류와 다중 분류로 나뉩니다. 이진 분류는 양성분류와 음성 분류로 나뉨

회귀는 연속적인 숫자 또는 프로그래밍 용어로 말하면 부동소수점수(수학 용어로는 실수)를 예측 하는 것 범위 예측도 포함 연속성으로 구분하면 됨 

모델이 처음보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 데스트 세트로 일반화(generalization) 되었다고 한다.

가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것을 과대적합이라고 한다.

너무 간단한 모델일 선택되는 것을 과소적합이라고 한다.

scikit-learn에 포함된 데이터셋은 Bunch 객체에 저장되어있음

Bunch 객체는 dictionary와 비슷하지만 점 표기법을 사용할 수있다.

결정 계수는 R제곱 값이라고도 하는데 회귀 일때 결졍계수를 반환한다.

결정계수는 회귀 모델에서 예측의 적합도를 측정한 것으로 0에서 1사이 값을 가짐

k-NN 의 장점은 쉬운 모델 단점은 데이터셋이 크면 시간이 오래 걸림 



머신러닝에서 알고리즘이 주어진 데이터로부터 학습하는 파라미터를 흔히 모델 파라미더라고 부름

모델이 학습할 수 없어서 사람이 직접 설정해 주어야 하는 파라미터를 하이퍼파라미터라고 부름

하이퍼파라미터는 파이썬 클래스와 함수에 넘겨주는 인수에 포함되므로 통칭하여 매개변수로 부름

선형회귀 도는 최소제곱법이라고 불리는 것은 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차를 최소화하는 파라미터 w와b를 찾는 것이다.

평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.

선형 회귀는 매개변수가 없는 것이 장점이지만 그래서 복잡도를 제어하지 못한다

기울기 파리미터(w)는 가중치(weight) 또는 계수(coefficient)라고 하며 lr 객체의 coef_ 속성에 저장되어 있고 편향(offset) 또는 절편(intercept)파리미터(b)는 intercept_ 속성에 저장되어있다.

intercept_ 속성은 항상 실수(float)값 하나지만, coef_속성은 각 입력 특성에 하나씩 대응되는 Numpy 배열이다.

기본 선형 회귀 식은 복잡도를 제어 할 수 없다

리지(Ridge)도 회귀를 위한 선형 모델이므로 최소적합법에서 사용한 것과 같은 예측 함수를 사용한다.

리지 회귀에서의 가중치(w) 선택은 훈련 데이터를 잘 예측하기 위해서 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적도 있다.

가중치의 절댓값을 가능한 한 작게 만드는 것이 목표이다.

w의 모든 원소가 0에 가깝게 되는 것이 목표, 이는 모든 특성이 출력에 주는 영향을 최소한으로 만드는 것이다. 이런 제약을 규제(regularization)이라고 한다.

규제란 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미이다.

리지 회귀에서 사용하는 규제 방식을 L2규제라고 한다.

수학적으로 리지는 계수의 L2노름(norm)의 제곱을 패널티로 적용한다.

Ridge 모델에서 alpha 매개변수로 훈련세트의 성능 대비 모델을 얼마나 단순화할지 지정 할 수 있다. alpha값을 높이면 계수를 0에 더 가깝게 만들어서 훈련세트의 성능은 나빠지지만 일반화에 도움을 줄 수 있다.

라소(lasso)도 리지 회귀와 같이 계수를 0에 가깝게 만들려고 한다. 이를 L1규제라고 한다. 라소는 어떤 계수를 정말 0으로 만들어서 모델에서 완전히 제거 시킨다.

라소와 리지 중 지리를 더 선호 하지만 특성이 많고 그중 일부분만 중요하다면 라소가 더 좋다. 라소와 리지를 결합한 ElasticNet도 있다.ElasticNet은 매개변수 두개를 조정해야한다.

회귀용 선형 모델에서는 출력이 특성의 선형 함수였다. 분류용 선형 모델에서는 결정 경계까 입력의 선형 함수이다.

손실함수(loss fuction)에 대한 차이는 중요하지 않다.

많이 알려진 두개의 선형 분류 알고리즘은 로지스틱 회귀(Logistic regression)과 서포트 벡터 머신(supoort vector machine)이다. 

Ridge와 마찬가지로 로지스틱 회귀와 서포트 벡터 머신은 L2 규제를 사용한다.

규제의 강도를 결정하는 매개 변수는 C이다. C값이 높아지면 규제가 감소한다. 즉, 매개변수로 높은 C값을 지정하면 훈련세트에 가능한 최대로 맞추려 노력하고, 반면에 C값을 낮추면 모델은 계수 벡터(w)가 0에 가까워지도록 만든다.

C의 값이 낮아지면 데이터 포인트 중 다수에 맞추려고 하는 반면, C의 값을 높이면 개개의 데이터 포인트를 정확히분류하려고 노력한다. 

선형 모델의 주요 매개 변수는 회귀 모델에서는 alpha 였고 LinearSVC와 LogisticRegression에서는 C이다. alpha 값이 클수록, C값이 작을수록 모델이 단순해 진다. 회귀 모델에서 이 매개 변수를 조정하는 일이 매주 종요하다. 중요한 특성이 많지 않다고 생각하면 L1구제를 사용한다. 그러지않으면 기본적으로 L2 규제를 사용한다. 

나이브 베이즈(naive bayes) 분류기는 로지스틱 리그레션이나 서포터 벡터 머신 같은 선형 분류기보다 훈련속도가 빠른 편이지만 그대신 일반화 성능이 조금 뒤쳐진다.

나이브 베이즈 분류기가 효과적인 이유는 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합하기 때문이다.

나이브 베이즈 분류기는 GaussianNB, BernoulliNB, MultinomialNB 세가지다.

가우시안은 연속적인 어떤데이터에도, 베노울리는 이진 데이터, 멀티노미얼은 카운트 데이터에 적용된다.

decision tree 는 스무고개 형식이다. 땅따먹기 형식으로 루트노드에서 시작하여 분할된 영역이 한개의 타깃값만 가질때까지 계속 나눈다. 하나의 타깃으로만 이루어진 노드를 순수 노드라고한다. 

결정트리에서 과대적합을 막는 방법은 사전가지치기와 사후가지치기 또는 가지치기

