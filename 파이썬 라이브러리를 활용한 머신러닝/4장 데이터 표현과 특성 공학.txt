특정 애플리케이션에서 가장 적합한 데이터 표현을 찾는것을 특성 공학(feature engineering)이라 한다. 

범주형 변수를 표현하는데 가장 널리 쓰이는 방법은 원-핫-인코딩(one-hot-encoding)이다. 이를 원-아웃-오프-엔 코딩(one-out-of-N encoding) 혹은 가변수(dummy variable)라도고 한다. 

pandas에서는 get_dummies 함수를 사용해 데이터를 매우 쉽게 인고딩할 수 있다. get_dummies 함수는 객체타입(문자열 같은)이나 범주형을 가진 열을 자동으로 변환 해준다.

대부분의 애플리케이션에서 일부 특성은 범주형이고 일부는 연속형이다. OneHotEncoder는 모든 특성을 범주형이라고 가정하기 대문에 바로 적용 할 수 없다. ColumnTrnasformer 클래스가 필요한 이유가 여기 있다. 이클래스는 입력 데이터에 있는 열마다 다른 변환을 적용할 수 있다. 

연속형 데이터에 아주 강력한 선형 모델을 만드는 방법 하나는 한 특성을 여러 특성으로 나누는 구간 분할(bining)이다. (이산화라고도 한다.)

용량이 매우 크고 고차원 데이터셋이라 선형 모델을 사용해야 한다면 구간 분할이 모델 성능을 높이는데 아주 좋은 방법이 될 수 있다.

특별히 특성을 풍부하게 나타내는 또 하나의 방법은 원본 데이터에 상호작용(interaction)과 다항식(polynomial)을 추가하는 것이다. 

선형 모델과 신경망은 각 특성의 스케일과 분포에 밀접하게 연관되어 있다. 그리고 특성과 타깃값사이에 비선형성이 있다면 특히 선형회귀에서는 모델을 만들기가 어렵다. log나 exp 같은 함수를 사용하는 것은 편법이지만 특성을 정규 분포로 바꾸는데 쉽고 효과적인 방법이다. 이런 변환이 도움이 되는 전형적인 경우는 정수 카운트 데이터를 다룰때이다. 카운트에는 음수가 없으며 특별한 통계 패턴을 따르는 경우가 많다.

구간 분할, 다항식, 상호작용은 데이터가 주어진 상황에서 모델의 성능에 큰 형향을 줄 수 있다. 특별히 선형 모델이나 나이브 베이즈 모델 같은 덜 복잡한 모델일 경우이다. 반면에 트리기반 모델은 스스로 중요한 상호작용을 찾아 낼 수 있고 대부분의 경우 데이터를 명시적으로 변환하지 않아도 된다.

좋은 특성을 뽑아내기 위한 전략으로 일변량 통계(univariate statistics), 모델 기반 선택(model-based selection), 반복적 선택(iterative selection)이 있다. 이 방법들은 모두 지도학습 방법이므로 최적값을 찾으렴녀 타깃값이 필요하다.

일변량 통계에서는 개개의 특성과 타깃 사이에 중요한 통계적 관계가 있는 지를 계산한다. 그 다음 깊게 관련되어 있다고 판단되는 특성을 선택한다. 분류에서는 분산분석(ANOVA, analysis of variance)라고도 한다.  이 방법의 핵심 요소는 일변량, 즉 각 특성이 독립적으로 평가된다는 점이다. 

모델 기반 특성 선택은 지도 학습 머신러닝 모델을 사용하여 특성의 중요도를 평가해서 가장 중요한 특성들만 선택합니다. 특성 선택을 위한 모델은 각 특성의 중요도를 측정하여 순서를 매길 수 있어야 한다. 결정트리 류 모델은 각 특성의 중요도가 담겨있는 featrue_importances_ 속성을 제공한다. 선형 모델의 절댓값도 특성의 중요도를 재는데 사용할 수 있다. 일변량 분석과는 반대로 모델 기반 특성 선택은 한번에 모든 특성을 고려하므로(사용된 모델이 상호작용을 잡아낼 수 있다면 ) 상호 작용 부분을 반영할 수도 있다. 

반복적 특성 선택(iterative feature selection)에서는 특성의 수가 각기 다른 일련의 모델이 만들어진다. 기본적으로 두가지 방법이 있다. 특성을 하나도 선택하지 않은 상태로 시작해서 어던 종료 조건에 도달할 때까지 하나씩 추가하는 방법이다. 두번째는 모든 특성을 가지고 시작해서 어던 종료 조건이 될 때까지 특성을 하나씩 제거해가는 방법이다. 재귀적 특성 제거(RFE, recursive feature elimination)이 이런 방법의 하나이다. 이 방법은 모든 특성으로 시작해서 모델을 만들고 특성 중요도가 가장 낮은 특성을 제거한다. 그런 다음 제거한 특성앨 빼고 나머지 특성 전체로 새로운 모델을 만든다. 이런 식으로 미리 정의된 특성 개수가 남을 때까지 계속한다.  이를 위해 모델 기반 선택에서 처럼 특성 선택에 사용할 모델은 특성의 중요도를 결정하는 방법을 제공해야 한다.

