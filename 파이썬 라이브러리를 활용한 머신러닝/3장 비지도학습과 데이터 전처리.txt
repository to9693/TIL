비지도 학습(unsupervised learning)이란 알고 있는 출력값이나 정보 업이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝을 가리킨다. 

두가지 비지도 학습이있다. 데이터의 비지도 변환(unsupervised transformation)과 군집(clustering)이다.

비지도 변환 (unsupervised transpormation) 은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터 보다 쉽게 해석 할 수 있도록 만드는 알고리즘이다.비지도 변환이 널리 사용되는 분야는 특성이 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소(dimencionality reduction)이다. 차원축소의 대표적인 예는 시각화를 위해 데이터셋을 2차원으로 변경하는 경우이다.

비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 한다. 많은 텍스트 문서에서 주제를 추출하는 것이 이런 예이다. 이때 처리할 작업은 문서에서 이야기하는 주제들이 무엇인지 찾고 학습하는 것이다. 이는 소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용할 수 있다.

군집 알고리즘은 데이터를 비슷한 것끼리 그룹을 묶는 것이다. 소셜 미디어 사이트에서 사진을 업로드 하는 예를 생각해 보자. 업로드한 사진을 분류하려면 같은 사람이 찍힌 사진을 같은 그룹으로 묶을 수 있다. 

비지도 학습에서 가장 어려운 일은 알고리즘이 뭔가 유용한 것을 학습했는지 평가하는 것이다. 비지도 학습은 보통 레이블이 없는 데이터에 적용하기 때문에 무엇이 올바른 출력인지 모른다. 그래서 어떤 모델이 일을 잘하고 있는지 이야기하기가 매우 어렵다. 

이런 이유로 비지도 학습 알고리즘은 데이터 과학자가 데이터를 더 잘 이해하고 싶을 때 탐색적 분석단계에서 많이 사용한다. 

비지도 학습은 지도 학습의 전처리 단계에서도 사용한다. 비지도 학습의 결과로 새롭게 표현된 데이터를 사용해 학습하면 지도 학습의 정확도가 좋아지기도 하고 메모리와 시간을 절약 할 수 있다. 

4가지의 전처리 방법이 있다. StandardScaler, MinMaxScaler, RobustScaler, Normalizer

StandardScaler 는 각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 한다. 특성의 최솟값과 최댓값 크기를 제한하지는 않는다. 변환된 값을 표준 점수, 표준 값, z- 점수라고 한다.

RobustScaler는 특성들이 같은 스케일을 갖게 된다는 통계적 측면에서는 StandardScaler와 시슷하다. 하지만 평균과 분산 대신 중간값(median)과 사분위 값(quartile)을 사용한다. 때문에 전체 데이터와 아주 동떨어진 데이터 포인트에 영향을 밪지 않는다. 이러한 이상데이터를 이상치(outlier)라고 한다.

MinMaxScaler는 모든 특성이 정확하게 0과 1사이에 위치하도록 데이터를 변경한다. 2차원 데이터셋일 경우 모든 데이터가 x축의 0과1, y축의 0과 1사이에 위치한다.

Normalizer는 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정한다.지름이 1인 원(3차원일 땐 구)에 데이터 포인트를 투영한다. 각 데이터포인트가 다른 비율로 (길이에 반비례하여) 스케일이 조정된다는 뜻이다. 이러한 정규화(nomalization)는 특성 벡터의 길이는 상관없고 데이터의 방향(또는 각도)만이 중요할 때 많이 사용한다.

MinMaxScaler 에서 테스트 세트와 트레인 세트 둘다 스케일을 해주는데 테스트 세트도 트레인 세트의 최댓값과 최솟값을 사용하기 때문에 테스트 세는 음수나 1이상의 수치가 나올 수 있다.

QuantileTransformerm 변환기가 추가되었다. QuantileTransformer는 기본적으로 1,000개의 분위(quantile)를 사용하여 데이터를 균등하게 분포시킨다. Robustscaler와 비슷하게 이상치에 민감하지 않으며 전체 데이터를 0과 1사이로 압축 한다.

분위수는 n_quantiles 매개변수에서 설정가능하고 기본값은 1,000이다. scaler 객체의 quantiles 속성에는 특성별로 계산된 분위 값이 들어있으므로 이속성의 크기는 (n_quantiles, n_features)이다. 

QuantileTransform는 output_distribution 매개변수에서 normal로 지졍하여 균등 분포가 아니라 정규 분포로 출력을 바꿀수 있다.

PowerTransformer는 데이터의 특성별로 정규분포 형태에 가깝도록 변환해준다. PowerTransformer은 method 매개변수에 'yeo-johson'와 'box-cox' 알고리즘을 지정할 수 있다. 기본 값은 'yeo-johnson'이다. 

보통 어떤 데이터셋에 fit을 적용하면 transform을 호출한다. 이럴 때를 위해 fit_transform 메서드를 제공한다.

주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터 셋을 회전시키는 기술이다. 회전한 뒤에 데이터를 설명하는 데 얼마나 중요하냐에 따라 종종 새로운 특성 중 일부만 선택된다. 

PCA의 단점은 그래프의 두축을 해석하기가 쉽지 않다.

PCA는 특성 추출에도 이용한다. 특성 추출은 원본 데이터 표현보다 분석하기에 더 적합한 표현을 찾을 수 있으리란 생각에서 출발한다. 이미지는 적색, 녹생, 청색(RGB)의 강도가 기록된 픽셀로 구성된다. 

NMF(non-negative matrix factorization)는 유용한 특성을 뽑아내기 위한 또 다른 학습 알고리즘이다. NMF는 음수가 아닌 성분과 계수 값을 찾는다. 음수가 아닌 가중치 합으로 데이터를 분해하는 기능은 여러 사람의 목소리가 담긴오디오 트랙이나 여러 악기로 이뤄진 음악처럼 독립된 소스를 추가하여(덮어써서) 만들어진 데이터에 특히 유용하다. NMF는 섞어 있는 데이터에서 원본 성분을 구분 할 수 있다. 

데이터를 산점도로 시각화 할 수 있다는 이점 때문에 PCA 가 데이터 변환에 가장 먼저 시도해볼 만한 방법이지만, 알고리즘의 (회전하고 방향을 제거하는)  태생상 유용성이 떨어진다. 매니폴드 학습(manifold learning)알고리즘이라고 하는 시각화 알고리즘은 훨씬 복잡한 매핑을 만들어 더 나은 시각화를 제공한다. 특히 t-SNE알고리즘을 많이 사용한다.

매니폴드 학습 알고리즘은 그 목적이 시각화라 3개 이상의 특성을 뽄느 경우는 거의 없다. t-SNE를 포함해서 일부 매니폴드 알고리즘들은 훈련 데이터를 새로운 표현으로 변환시키지만 새로운 데이터에는 적용하지 못한다. 그래서 매니폴드 학습은 탐색적 데이터 분석에 유용하지만 지도 학습용으로는 거의 사용하지 않는다. 

t-SNE 의 아이디어는 데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는 것이다. 먼저 t-SNE는 각 데이터 표인트를 2차원에 무작위로 표현한 후 원본 특성 공간에서 가까운 포인트를 가깝게, 멀리 떨어진 포인는 멀어지게 만든다. 멀리 떨어진 포인트와 거리를 보존하는 것보다 가까이 있는 포인트에 더 많은 비중을 둔다. 이웃 데이터 포인트에 대한 정보를 보존하려고 노력한다.

군집(clustering)은 데이터셋을 클러스터(cluster)라는 그룹으로 나누는 작업이다. 한 클러스터 안의 데이터 포인트 끼리는 매우 비슷하고 다른 클러스터의 데이터 포인트와는 구분되도록 데이터를 나누는 것이 목표이다. 분류 알고리즘과 비슷하게 군집 알고리즘은 각 데이터 포인트가 어느 클러스터에 속하는지 할당(또는 예측)한다.

k-평균(k-means)군집은 가장 간단하고 또 널리 사용하는 군집알고리즘이다. 데이터의 어떤 영역을 대표하는 클러스터 중심(cluster center)를 찾는다. 먼저 데이터 포인트를 가장 가까운 클러스터 중심에 할당하고, 그런 다음 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정한다. 클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘이 종료된다. 

