비지도 학습(unsupervised learning)이란 알고 있는 출력값이나 정보 업이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝을 가리킨다. 

두가지 비지도 학습이있다. 데이터의 비지도 변환(unsupervised transformation)과 군집(clustering)이다.

비지도 변환 (unsupervised transpormation) 은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터 보다 쉽게 해석 할 수 있도록 만드는 알고리즘이다.비지도 변환이 널리 사용되는 분야는 특성이 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소(dimencionality reduction)이다. 차원축소의 대표적인 예는 시각화를 위해 데이터셋을 2차원으로 변경하는 경우이다.

비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 한다. 많은 텍스트 문서에서 주제를 추출하는 것이 이런 예이다. 이때 처리할 작업은 문서에서 이야기하는 주제들이 무엇인지 찾고 학습하는 것이다. 이는 소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용할 수 있다.

군집 알고리즘은 데이터를 비슷한 것끼리 그룹을 묶는 것이다. 소셜 미디어 사이트에서 사진을 업로드 하는 예를 생각해 보자. 업로드한 사진을 분류하려면 같은 사람이 찍힌 사진을 같은 그룹으로 묶을 수 있다. 

비지도 학습에서 가장 어려운 일은 알고리즘이 뭔가 유용한 것을 학습했는지 평가하는 것이다. 비지도 학습은 보통 레이블이 없는 데이터에 적용하기 때문에 무엇이 올바른 출력인지 모른다. 그래서 어떤 모델이 일을 잘하고 있는지 이야기하기가 매우 어렵다. 

이런 이유로 비지도 학습 알고리즘은 데이터 과학자가 데이터를 더 잘 이해하고 싶을 때 탐색적 분석단계에서 많이 사용한다. 

비지도 학습은 지도 학습의 전처리 단계에서도 사용한다. 비지도 학습의 결과로 새롭게 표현된 데이터를 사용해 학습하면 지도 학습의 정확도가 좋아지기도 하고 메모리와 시간을 절약 할 수 있다. 

4가지의 전처리 방법이 있다. StandardScaler, MinMaxScaler, RobustScaler, Normalizer

StandardScaler 는 각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 한다. 특성의 최솟값과 최댓값 크기를 제한하지는 않는다. 변환된 값을 표준 점수, 표준 값, z- 점수라고 한다.

RobustScaler는 특성들이 같은 스케일을 갖게 된다는 통계적 측면에서는 StandardScaler와 시슷하다. 하지만 평균과 분산 대신 중간값(median)과 사분위 값(quartile)을 사용한다. 때문에 전체 데이터와 아주 동떨어진 데이터 포인트에 영향을 밪지 않는다. 이러한 이상데이터를 이상치(outlier)라고 한다.

MinMaxScaler는 모든 특성이 정확하게 0과 1사이에 위치하도록 데이터를 변경한다. 2차원 데이터셋일 경우 모든 데이터가 x축의 0과1, y축의 0과 1사이에 위치한다.

Normalizer는 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정한다.지름이 1인 원(3차원일 땐 구)에 데이터 포인트를 투영한다. 각 데이터포인트가 다른 비율로 (길이에 반비례하여) 스케일이 조정된다는 뜻이다. 이러한 정규화(nomalization)는 특성 벡터의 길이는 상관없고 데이터의 방향(또는 각도)만이 중요할 때 많이 사용한다.

MinMaxScaler 에서 테스트 세트와 트레인 세트 둘다 스케일을 해주는데 테스트 세트도 트레인 세트의 최댓값과 최솟값을 사용하기 때문에 테스트 세는 음수나 1이상의 수치가 나올 수 있다.

QuantileTransformerm 변환기가 추가되었다. QuantileTransformer는 기본적으로 1,000개의 분위(quantile)를 사용하여 데이터를 균등하게 분포시킨다. Robustscaler와 비슷하게 이상치에 민감하지 않으며 전체 데이터를 0과 1사이로 압축 한다.

분위수는 n_quantiles 매개변수에서 설정가능하고 기본값은 1,000이다. scaler 객체의 quantiles 속성에는 특성별로 계산된 분위 값이 들어있으므로 이속성의 크기는 (n_quantiles, n_features)이다. 

QuantileTransform는 output_distribution 매개변수에서 normal로 지졍하여 균등 분포가 아니라 정규 분포로 출력을 바꿀수 있다.

PowerTransformer는 데이터의 특성별로 정규분포 형태에 가깝도록 변환해준다. PowerTransformer은 method 매개변수에 'yeo-johson'와 'box-cox' 알고리즘을 지정할 수 있다. 기본 값은 'yeo-johnson'이다. 

보통 어떤 데이터셋에 fit을 적용하면 transform을 호출한다. 이럴 때를 위해 fit_transform 메서드를 제공한다.

주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터 셋을 회전시키는 기술이다. 회전한 뒤에 데이터를 설명하는 데 얼마나 중요하냐에 따라 종종 새로운 특성 중 일부만 선택된다. 

PCA의 단점은 그래프의 두축을 해석하기가 쉽지 않다.

PCA는 특성 추출에도 이용한다. 특성 추출은 원본 데이터 표현보다 분석하기에 더 적합한 표현을 찾을 수 있으리란 생각에서 출발한다. 이미지는 적색, 녹생, 청색(RGB)의 강도가 기록된 픽셀로 구성된다. 

NMF(non-negative matrix factorization)는 유용한 특성을 뽑아내기 위한 또 다른 학습 알고리즘이다. NMF는 음수가 아닌 성분과 계수 값을 찾는다. 음수가 아닌 가중치 합으로 데이터를 분해하는 기능은 여러 사람의 목소리가 담긴오디오 트랙이나 여러 악기로 이뤄진 음악처럼 독립된 소스를 추가하여(덮어써서) 만들어진 데이터에 특히 유용하다. NMF는 섞어 있는 데이터에서 원본 성분을 구분 할 수 있다. 

데이터를 산점도로 시각화 할 수 있다는 이점 때문에 PCA 가 데이터 변환에 가장 먼저 시도해볼 만한 방법이지만, 알고리즘의 (회전하고 방향을 제거하는)  태생상 유용성이 떨어진다. 매니폴드 학습(manifold learning)알고리즘이라고 하는 시각화 알고리즘은 훨씬 복잡한 매핑을 만들어 더 나은 시각화를 제공한다. 특히 t-SNE알고리즘을 많이 사용한다.

매니폴드 학습 알고리즘은 그 목적이 시각화라 3개 이상의 특성을 뽄느 경우는 거의 없다. t-SNE를 포함해서 일부 매니폴드 알고리즘들은 훈련 데이터를 새로운 표현으로 변환시키지만 새로운 데이터에는 적용하지 못한다. 그래서 매니폴드 학습은 탐색적 데이터 분석에 유용하지만 지도 학습용으로는 거의 사용하지 않는다. 

t-SNE 의 아이디어는 데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는 것이다. 먼저 t-SNE는 각 데이터 표인트를 2차원에 무작위로 표현한 후 원본 특성 공간에서 가까운 포인트를 가깝게, 멀리 떨어진 포인는 멀어지게 만든다. 멀리 떨어진 포인트와 거리를 보존하는 것보다 가까이 있는 포인트에 더 많은 비중을 둔다. 이웃 데이터 포인트에 대한 정보를 보존하려고 노력한다.

군집(clustering)은 데이터셋을 클러스터(cluster)라는 그룹으로 나누는 작업이다. 한 클러스터 안의 데이터 포인트 끼리는 매우 비슷하고 다른 클러스터의 데이터 포인트와는 구분되도록 데이터를 나누는 것이 목표이다. 분류 알고리즘과 비슷하게 군집 알고리즘은 각 데이터 포인트가 어느 클러스터에 속하는지 할당(또는 예측)한다.

k-평균(k-means)군집은 가장 간단하고 또 널리 사용하는 군집알고리즘이다. 데이터의 어떤 영역을 대표하는 클러스터 중심(cluster center)를 찾는다. 먼저 데이터 포인트를 가장 가까운 클러스터 중심에 할당하고, 그런 다음 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정한다. 클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘이 종료된다. 

```python
noise = X_people[labels==-1]

fig,axes = plt.subplots(3, 9, subplot_kw = {'xticks': (), 'yticks': ()},
                       figsize=(12,4))
for image, ax in zip(noise, axes.ravel()):
    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)
```



k-평균의 단점은 무작위 초기화를 사용하여 알고리즘의 출력이 난수 초깃값에 따라 달라진다. 또한 클러스터 모양을 가정하고 있어서 활용 범위가 제한적이고, 찾으려 하는 클래스터 개수를 지정해야 한다.

병합군집(agglomerative clustering)은 시작 할 때 각포인트를 하나의 클러스터로 지정하고, 그 다음 어떤 종류 조건을 만족할 때까지 가장 비슷한두 클러스터를 합쳐나간다. scikit-learn에서 사용하는 종료 조건은 클러스터 개수로, 지정된 개수의 클러스터가 남을 때까지 비슷한 클러스터를 합친다. linkage 옵션에서 가장 비슷한 클러스터를 측정하는 방법을 지정한다.

병합군집의 옵션 

ward -기본값으로 모든 클러스터 내의 분산을 가장 작게 증가시키는 두클러스터를 합친다.

average - 클러스터 포인트 사이의 평균거리가 가장 짧은 것

complete - 클러스터 포인트 사이의 최대거리가 가장 짧은 두 클러스터를 합친다.

single - 클러스터 포인트 사ㅣ의 최소거리가 가장 짧은 두 클러스터를 합친다.

병합군집은 계층적 군집(hierarchical clustering)을 만든다. 군집이 반복하여 진행되면 모든 포인트는 하나의 포인트를 가진 클러스터에서 시작하여 마지막 클러스터까지 이동한다. 각 중간 단계는 데이터에 대한 클러스터를 생성한다. 이는 가능한 모든 클러스터를 연결해보는데 도움이 된다. 덴드로그램(dendrogram)을 사용하여 다차원 데이터셋의 병합군집 과정을 그려볼 수 있다.

덴드로그램을 보면 군집되는 순서와 군집 할 때 거리가 상대적으로 멀었는지도 알수 있다.

DBSCAN(density-based spatial clustering of applications with noise) 의 주요 장점은 클러스터의 개수를 미리 지정할 필요가 없다는 것이다. 복잡한 형상도 찾을 수 있으며, 어떤 클래스에도 속하지 않는 포인트를 구분할 수 있다. DBSCAN은 특성 공간에서 가까이 있는 데이터가 많아 붐비는 지역의 포인트를 찾는다. 이런 지역을 특성 공간의 밀집지역(dense region)이라고 한다. DBSCAN의 아이디어는 데이터의 밀집지역이 한 클러스터를 구성하며 비교적 비어있는 지역을 경계로 다른 클러스터와 구분된다는 것이다. 

밀집 지역에 있는 포인트를 핵심샘플(또는 핵심 포인트)라고 하며 다음과 같이 정의한다. DBSCAN에는 두 개의 매개변수 min_samples와 eps 가 있다. 한 데이터 포인트에서 eps 거리 안에 데이터가 min_samples 개수 만큼 들어 있으면 이 데이터 포인트를 핵심 샘플로 분류 한다. eps보다 가까운 핵심 샘플은 DBSCAN에 의해 동일한 클러스터로 합친다. 

군집 알고리즘의 결과를 실제 정답 클러스터와 비교하여 평가할 수 있는 지표들이 있다. 1(최적일 때)과 0(무작위로 분류 될때) 사이의 값을 제공하는 ARI(adjusted rand index)와 NMI(normalized mutual information)가 가장 널리 사용하는 지표이다. (ARI는 음수가 될 수 있다.)

군집 모델을 평가할 때 흔히 하는 실수가 adjusted_rand_score 나 nomalized_mutual_info_score 같은 군집용 축정 도구를 사용하지 않고 accuracy_score를 사용하는 것이다. 정확도를 사용하면 할당된 클러스터의 레이블 이름이 실제 레이블 이름과 맞는지 확인한다. 그러나 클러스터 레이블은 그자체로 의미가 있는 것이 아니며 포인트들이 같은 클러스터에 속해 있는 가만이 중요하다.

ARI나 NMI 같은 지표는 애플리케이션의 성능 평가가 아니라 알고리즘을 개발할 때 도움이 된다. 타깃값이 필요 없는 군집용 지표로 실루엣 계수(silhouette coefficient)가 있다. 실제로 잘 작동하지는 않는다.

k-평균과 병합 군집은 원하는 클러스터 개수를 지정할 수 있고 DBSCAN은 eps 매개변수를 사용하여 클러스터 크기를 간접적으로 조절 할 수 있다. 이 세 모델은 실제 대량의 데이터셋에 사용할 수 있고 비교적 쉽게 이해할 수 있으며 여러 개의 클러스터로 군집을 만들 수있다.

k-평균의 장점은 클러스터 중심으로 사용해 클러스터를 구분한다. 이 알고리즘은 가가 데이터 포인트를 클러스터 중심으로 대표할 수 있기 때문에 분해 방법으로 볼 수 있다.

DBSCAN의 장점은 클러스터에 할당되지 않는 잡음 포인트를 인식할 수 있으며, 클러스터의 개수를 자동으로 결정한다. 다른 두 알고리즘과 달리 two_moons 예에서처럼 복잡한 클러스터의 모양을 인식할 수 있다. DBSCAN은 크기가 많이 다른 클러스터를 만들어 내곤 하는데 장점이기도 하고 단점이 될 수 도 있다.

병합 군집은 전체 데이터의 분할 계층도를 만들어 주며 덴드로그램을 사용해 쉽게 확인할 수 있다.

